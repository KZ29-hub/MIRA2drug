{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a8b487-e51f-471b-95c9-d532bc64045e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers: ['SMILES', 'Protein', 'Y']\n",
      "headers: ['SMILES', 'Protein', 'Y']\n",
      "headers: ['DBID', 'Drugname', 'Description']\n",
      "headers: ['drugbank_id', 'name', 'smiles']\n",
      "train length 3453\n",
      "test length 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train dataset: 100%|██████████| 3453/3453 [08:16<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_length 3453\n",
      "Filtered train_length 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test dataset: 100%|██████████| 311/311 [00:44<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_length 311\n",
      "Filtered train_length 1\n",
      "All OK!\n"
     ]
    }
   ],
   "source": [
    "### drug+protein random-only\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import csv\n",
    "import jsonlines\n",
    "\n",
    "def readJSONL(fp):\n",
    "    res = []\n",
    "    with open(fp,\"r\",encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res  \n",
    "\n",
    "def writeJSONL(instance,fp):\n",
    "    with jsonlines.open(fp,'w') as f:\n",
    "        for sample in instance:\n",
    "            f.write(sample)\n",
    "\n",
    "\n",
    "\n",
    "def readCSV_data(fp):\n",
    "    with open(fp, mode='r', encoding='us-ascii', errors='ignore') as file:\n",
    "        # 使用 csv.reader 来处理逗号分隔的内容\n",
    "        reader = csv.reader(file)\n",
    "        # 将读取的内容转换为列表\n",
    "        lines = list(reader)\n",
    "\n",
    "        # 处理标题行\n",
    "        headers = lines[0][:3]  # 取前3个标题\n",
    "        print('headers:', headers)\n",
    "\n",
    "        # 生成字典列表\n",
    "        data_list = []\n",
    "        for values in lines[1:]:  # 从第二行开始\n",
    "            if len(values) < len(headers):\n",
    "                print(f\"Warning: Skipped line due to mismatch in length: {values}\")\n",
    "                continue\n",
    "            # 创建字典并添加到列表\n",
    "            row_dict = {headers[i]: values[i] for i in range(len(headers))}\n",
    "            data_list.append(row_dict)\n",
    "        return data_list\n",
    "\n",
    "def readCSV_desc(fp):\n",
    "    with open(fp, mode='r', encoding='us-ascii', errors='ignore') as file:\n",
    "        # 使用 csv.reader 来处理逗号分隔的内容\n",
    "        reader = csv.reader(file)\n",
    "        # 将读取的内容转换为列表\n",
    "        lines = list(reader)\n",
    "\n",
    "        # 处理标题行\n",
    "        headers = lines[0][:3]  # 取前3个标题\n",
    "        print('headers:', headers)\n",
    "\n",
    "        # 生成字典列表\n",
    "        data_list = []\n",
    "        for values in lines[1:]:  # 从第二行开始\n",
    "            if len(values) < len(headers):\n",
    "                print(f\"Warning: Skipped line due to mismatch in length: {values}\")\n",
    "                continue\n",
    "            # 创建字典并添加到列表\n",
    "            row_dict = {headers[i]: values[i] for i in range(len(headers))}\n",
    "            data_list.append(row_dict)\n",
    "\n",
    "    return data_list\n",
    "    \n",
    "def filter_DBID_by_smiles(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('smiles') == key_value]\n",
    "\n",
    "def filter_desc_by_DBID(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('DBID') == key_value]\n",
    "\n",
    "def filter_desc_by_sequence(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('Sequence') == key_value]\n",
    "    \n",
    "def human_transform(input_dir, output_dir, desc_path, smile_path, protein_desc_path):\n",
    "    ## 50 100 500\n",
    "    random.seed(500)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_path = os.path.join(input_dir, \"train.csv\")\n",
    "    test_path = os.path.join(input_dir, \"test.csv\")\n",
    "    ou_train_path = os.path.join(output_dir, \"train.jsonl\")\n",
    "    ou_test_path = os.path.join(output_dir, \"test.jsonl\")\n",
    "    \n",
    "    \n",
    "    ### SMILES, Protein,  Y\n",
    "    train_json = readCSV_data(train_path)\n",
    "    test_json = readCSV_data(test_path)\n",
    "    ### DBID, Drugname, Description\n",
    "    desc_json = readCSV_desc(desc_path)\n",
    "    ### drugbank_id, name, smiles\n",
    "    smile_json = readCSV_data(smile_path)\n",
    "    print('train length', len(train_json))\n",
    "    print('test length', len(test_json))\n",
    "\n",
    "    protein_desc = readJSONL(protein_desc_path)\n",
    "    \n",
    "    train_results = []\n",
    "    for example in tqdm(train_json, total=len(train_json), desc=\"Processing Train dataset\"):\n",
    "              \n",
    "        if int(float(example[\"Y\"])) == 1:\n",
    "            pos = \"advise\"\n",
    "            neg = [\"negative\"]\n",
    "        else:\n",
    "            pos = \"negative\"\n",
    "            neg = [\"advise\"]\n",
    "            \n",
    "        drug_DBID = filter_DBID_by_smiles(smile_json, example[\"SMILES\"])\n",
    "        protein_dict_list = filter_desc_by_sequence(protein_desc, example[\"Protein\"])\n",
    "        if drug_DBID and protein_dict_list:\n",
    "            drug_DBID = drug_DBID[0][\"drugbank_id\"]\n",
    "            drug_desc_dict = filter_desc_by_DBID(desc_json, drug_DBID)\n",
    "            \n",
    "            if drug_desc_dict:\n",
    "                drug_desc_dict = drug_desc_dict[0]\n",
    "                \n",
    "                protein_dict = protein_dict_list[0]\n",
    "                protein_text = f\"'Protein names':{protein_dict['Protein names']}, 'Gene Names':{protein_dict['Gene Names']}, 'Sequence':{protein_dict['Sequence']}, 'Features':{protein_dict['Features']}, 'Keywords':{protein_dict['Keywords']}\"\n",
    "                \n",
    "          \n",
    "                dti_item = {\n",
    "                        \"drug1_id\": drug_DBID,\n",
    "                        \"target_sequence\": example[\"Protein\"],\n",
    "                        \"drug1_name\": drug_desc_dict[\"Drugname\"],\n",
    "                        \"drug1_smile\": example[\"SMILES\"],\n",
    "                        \"drug1_desc\": drug_desc_dict[\"Description\"],\n",
    "                        \"protein_desc\": protein_text,\n",
    "                        \"pos\": pos,\n",
    "                        \"neg\": neg\n",
    "                    }\n",
    "                # print(dti_item)\n",
    "                train_results.append(dti_item)\n",
    "        #     else:\n",
    "        #         dti_item = {\n",
    "        #                 \"drug1_id\": \"\",\n",
    "        #                 \"target_sequence\": example[\"Protein\"],\n",
    "        #                 \"drug1_name\": \"\",\n",
    "        #                 \"drug1_smile\": example[\"SMILES\"],\n",
    "        #                 \"drug1_desc\": \"\",\n",
    "        #                 \"pos\": pos,\n",
    "        #                 \"neg\": neg\n",
    "        #             }\n",
    "            \n",
    "        #         train_results.append(dti_item)\n",
    "                \n",
    "        # else:\n",
    "        #     dti_item = {\n",
    "        #             \"drug1_id\": \"\",\n",
    "        #             \"target_sequence\": example[\"Protein\"],\n",
    "        #             \"drug1_name\": \"\",\n",
    "        #             \"drug1_smile\": example[\"SMILES\"],\n",
    "        #             \"drug1_desc\": \"\",\n",
    "        #             \"pos\": pos,\n",
    "        #             \"neg\": neg\n",
    "        #         }\n",
    "        \n",
    "            # train_results.append(dti_item)\n",
    "    random.shuffle(train_results)\n",
    "    print(\"Original train_length\", len(train_json))\n",
    "    print(\"Filtered train_length\", len(train_results))\n",
    "    writeJSONL(train_results, ou_train_path)\n",
    "\n",
    "\n",
    "    test_results = []\n",
    "    for example in tqdm(test_json, total=len(test_json), desc=\"Processing Test dataset\"):\n",
    "        if int(float(example[\"Y\"])) == 1:\n",
    "            pos = \"advise\"\n",
    "            neg = [\"negative\"]\n",
    "        else:\n",
    "            pos = \"negative\"\n",
    "            neg = [\"advise\"]\n",
    "            \n",
    "        drug_DBID = filter_DBID_by_smiles(smile_json, example[\"SMILES\"])\n",
    "        protein_dict_list = filter_desc_by_sequence(protein_desc, example[\"Protein\"])\n",
    "        if drug_DBID and protein_dict_list:\n",
    "            drug_DBID = drug_DBID[0][\"drugbank_id\"]\n",
    "            drug_desc_dict = filter_desc_by_DBID(desc_json, drug_DBID)\n",
    "            \n",
    "            if drug_desc_dict:\n",
    "                drug_desc_dict = drug_desc_dict[0]\n",
    "                \n",
    "                protein_dict = protein_dict_list[0]\n",
    "                protein_text = f\"'Protein names':{protein_dict['Protein names']}, 'Gene Names':{protein_dict['Gene Names']}, 'Sequence':{protein_dict['Sequence']}, 'Features':{protein_dict['Features']}, 'Keywords':{protein_dict['Keywords']}\"\n",
    "                \n",
    "          \n",
    "                dti_item = {\n",
    "                        \"drug1_id\": drug_DBID,\n",
    "                        \"target_sequence\": example[\"Protein\"],\n",
    "                        \"drug1_name\": drug_desc_dict[\"Drugname\"],\n",
    "                        \"drug1_smile\": example[\"SMILES\"],\n",
    "                        \"drug1_desc\": drug_desc_dict[\"Description\"],\n",
    "                        \"protein_desc\": protein_text,\n",
    "                        \"pos\": pos,\n",
    "                        \"neg\": neg\n",
    "                    }\n",
    "                # print(dti_item)\n",
    "                test_results.append(dti_item)\n",
    "        #     else:\n",
    "        #         dti_item = {\n",
    "        #                 \"drug1_id\": \"\",\n",
    "        #                 \"target_sequence\": example[\"Protein\"],\n",
    "        #                 \"drug1_name\": \"\",\n",
    "        #                 \"drug1_smile\": example[\"SMILES\"],\n",
    "        #                 \"drug1_desc\": \"\",\n",
    "        #                 \"pos\": pos,\n",
    "        #                 \"neg\": neg\n",
    "        #             }\n",
    "            \n",
    "        #         test_results.append(dti_item)\n",
    "                \n",
    "        # else:\n",
    "        #     dti_item = {\n",
    "        #             \"drug1_id\": \"\",\n",
    "        #             \"target_sequence\": example[\"Protein\"],\n",
    "        #             \"drug1_name\": \"\",\n",
    "        #             \"drug1_smile\": example[\"SMILES\"],\n",
    "        #             \"drug1_desc\": \"\",\n",
    "        #             \"pos\": pos,\n",
    "        #             \"neg\": neg\n",
    "        #         }\n",
    "        \n",
    "        #     test_results.append(dti_item)\n",
    "    random.shuffle(test_results)\n",
    "    print(\"Original train_length\", len(test_json))\n",
    "    print(\"Filtered train_length\", len(test_results))\n",
    "    writeJSONL(test_results, ou_test_path)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    desc_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp/Drug_description_expand_upload.csv\"\n",
    "    smile_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp/df_drugbank_smiles_filtered.csv\"\n",
    "    protein_desc_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/protein_desc/protein_desc.jsonl\"\n",
    "    \n",
    "    input_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_ori/human/cold/\"\n",
    "    output_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/human_tmp/random_only/fold0/\"\n",
    "    \n",
    "\n",
    "    human_transform(input_dir, output_dir, desc_path, smile_path, protein_desc_path)\n",
    "\n",
    "    print(\"All OK!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372e2c23-ad29-4435-87b0-e5d8166d860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headers: ['SMILES', 'Protein', 'Y']\n",
      "headers: ['SMILES', 'Protein', 'Y']\n",
      "headers: ['DBID', 'Drugname', 'Description']\n",
      "headers: ['drugbank_id', 'name', 'smiles']\n",
      "train2 length 3628\n",
      "test length 907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train2 dataset: 100%|██████████| 3628/3628 [08:44<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_length 3628\n",
      "Filtered train_length 1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test dataset: 100%|██████████| 907/907 [02:11<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_length 907\n",
      "Filtered train_length 419\n",
      "All OK!\n"
     ]
    }
   ],
   "source": [
    "### drug+protein cluster-only\n",
    "#### cluster - only - correct 只处理target的\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import csv\n",
    "import jsonlines\n",
    "\n",
    "def readJSONL(fp):\n",
    "    res = []\n",
    "    with open(fp,\"r\",encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res  \n",
    "\n",
    "def writeJSONL(instance,fp):\n",
    "    with jsonlines.open(fp,'w') as f:\n",
    "        for sample in instance:\n",
    "            f.write(sample)\n",
    "\n",
    "\n",
    "\n",
    "def readCSV_data(fp):\n",
    "    with open(fp, mode='r', encoding='us-ascii', errors='ignore') as file:\n",
    "        # 使用 csv.reader 来处理逗号分隔的内容\n",
    "        reader = csv.reader(file)\n",
    "        # 将读取的内容转换为列表\n",
    "        lines = list(reader)\n",
    "\n",
    "        # 处理标题行\n",
    "        headers = lines[0][:3]  # 取前3个标题\n",
    "        print('headers:', headers)\n",
    "\n",
    "        # 生成字典列表\n",
    "        data_list = []\n",
    "        for values in lines[1:]:  # 从第二行开始\n",
    "            if len(values) < len(headers):\n",
    "                print(f\"Warning: Skipped line due to mismatch in length: {values}\")\n",
    "                continue\n",
    "            # 创建字典并添加到列表\n",
    "            row_dict = {headers[i]: values[i] for i in range(len(headers))}\n",
    "            data_list.append(row_dict)\n",
    "        return data_list\n",
    "\n",
    "def readCSV_desc(fp):\n",
    "    with open(fp, mode='r', encoding='us-ascii', errors='ignore') as file:\n",
    "        # 使用 csv.reader 来处理逗号分隔的内容\n",
    "        reader = csv.reader(file)\n",
    "        # 将读取的内容转换为列表\n",
    "        lines = list(reader)\n",
    "\n",
    "        # 处理标题行\n",
    "        headers = lines[0][:3]  # 取前3个标题\n",
    "        print('headers:', headers)\n",
    "\n",
    "        # 生成字典列表\n",
    "        data_list = []\n",
    "        for values in lines[1:]:  # 从第二行开始\n",
    "            if len(values) < len(headers):\n",
    "                print(f\"Warning: Skipped line due to mismatch in length: {values}\")\n",
    "                continue\n",
    "            # 创建字典并添加到列表\n",
    "            row_dict = {headers[i]: values[i] for i in range(len(headers))}\n",
    "            data_list.append(row_dict)\n",
    "\n",
    "    return data_list\n",
    "    \n",
    "def filter_DBID_by_smiles(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('smiles') == key_value]\n",
    "\n",
    "def filter_desc_by_DBID(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('DBID') == key_value]\n",
    "\n",
    "def filter_desc_by_sequence(data_list, key_value):\n",
    "    return [d for d in data_list if d.get('Sequence') == key_value]\n",
    "\n",
    "def human_transform(input_dir, output_dir, desc_path, smile_path, protein_desc_path):\n",
    "    ## 50, 100,500\n",
    "    random.seed(500)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # train_path = os.path.join(input_dir, \"source_train.csv\")\n",
    "    train2_path = os.path.join(input_dir, \"target_train.csv\")\n",
    "    test_path = os.path.join(input_dir, \"target_test.csv\")\n",
    "    \n",
    "    ou_train_path = os.path.join(output_dir, \"train.jsonl\")\n",
    "    ou_test_path = os.path.join(output_dir, \"test.jsonl\")\n",
    "    \n",
    "    \n",
    "    ### SMILES, Protein,  Y\n",
    "    # train_json = readCSV_data(train_path)\n",
    "    train2_json = readCSV_data(train2_path)\n",
    "\n",
    "    test_json = readCSV_data(test_path)\n",
    "    ### DBID, Drugname, Description\n",
    "    desc_json = readCSV_desc(desc_path)\n",
    "    ### drugbank_id, name, smiles\n",
    "    smile_json = readCSV_data(smile_path)\n",
    "    print('train2 length', len(train2_json))\n",
    "    print('test length', len(test_json))\n",
    "\n",
    "    protein_desc = readJSONL(protein_desc_path)\n",
    "    \n",
    "    train_results = []\n",
    "\n",
    "            \n",
    "    for example in tqdm(train2_json, total=len(train2_json), desc=\"Processing Train2 dataset\"):\n",
    "        if int(float(example[\"Y\"])) == 1:\n",
    "            pos = \"advise\"\n",
    "            neg = [\"negative\"]\n",
    "        else:\n",
    "            pos = \"negative\"\n",
    "            neg = [\"advise\"]\n",
    "            \n",
    "        drug_DBID = filter_DBID_by_smiles(smile_json, example[\"SMILES\"])\n",
    "        protein_dict_list = filter_desc_by_sequence(protein_desc, example[\"Protein\"])\n",
    "        if drug_DBID and protein_dict_list:\n",
    "            drug_DBID = drug_DBID[0][\"drugbank_id\"]\n",
    "            drug_desc_dict = filter_desc_by_DBID(desc_json, drug_DBID)\n",
    "            \n",
    "            if drug_desc_dict:\n",
    "                drug_desc_dict = drug_desc_dict[0]\n",
    "                \n",
    "                protein_dict = protein_dict_list[0]\n",
    "                protein_text = f\"'Protein names':{protein_dict['Protein names']}, 'Gene Names':{protein_dict['Gene Names']}, 'Sequence':{protein_dict['Sequence']}, 'Features':{protein_dict['Features']}, 'Keywords':{protein_dict['Keywords']}\"\n",
    "                \n",
    "          \n",
    "                dti_item = {\n",
    "                        \"drug1_id\": drug_DBID,\n",
    "                        \"target_sequence\": example[\"Protein\"],\n",
    "                        \"drug1_name\": drug_desc_dict[\"Drugname\"],\n",
    "                        \"drug1_smile\": example[\"SMILES\"],\n",
    "                        \"drug1_desc\": drug_desc_dict[\"Description\"],\n",
    "                        \"protein_desc\": protein_text,\n",
    "                        \"pos\": pos,\n",
    "                        \"neg\": neg\n",
    "                    }\n",
    "                # print(dti_item)\n",
    "                train_results.append(dti_item)           \n",
    "            \n",
    "    random.shuffle(train_results)\n",
    "    print(\"Original train_length\", len(train2_json))\n",
    "    print(\"Filtered train_length\", len(train_results))\n",
    "    writeJSONL(train_results, ou_train_path)\n",
    "\n",
    "\n",
    "    test_results = []\n",
    "    for example in tqdm(test_json, total=len(test_json), desc=\"Processing Test dataset\"):\n",
    "        if int(float(example[\"Y\"])) == 1:\n",
    "            pos = \"advise\"\n",
    "            neg = [\"negative\"]\n",
    "        else:\n",
    "            pos = \"negative\"\n",
    "            neg = [\"advise\"]\n",
    "            \n",
    "        drug_DBID = filter_DBID_by_smiles(smile_json, example[\"SMILES\"])\n",
    "        protein_dict_list = filter_desc_by_sequence(protein_desc, example[\"Protein\"])\n",
    "        if drug_DBID and protein_dict_list:\n",
    "            drug_DBID = drug_DBID[0][\"drugbank_id\"]\n",
    "            drug_desc_dict = filter_desc_by_DBID(desc_json, drug_DBID)\n",
    "            \n",
    "            if drug_desc_dict:\n",
    "                drug_desc_dict = drug_desc_dict[0]\n",
    "                \n",
    "                protein_dict = protein_dict_list[0]\n",
    "                protein_text = f\"'Protein names':{protein_dict['Protein names']}, 'Gene Names':{protein_dict['Gene Names']}, 'Sequence':{protein_dict['Sequence']}, 'Features':{protein_dict['Features']}, 'Keywords':{protein_dict['Keywords']}\"\n",
    "                \n",
    "          \n",
    "                dti_item = {\n",
    "                        \"drug1_id\": drug_DBID,\n",
    "                        \"target_sequence\": example[\"Protein\"],\n",
    "                        \"drug1_name\": drug_desc_dict[\"Drugname\"],\n",
    "                        \"drug1_smile\": example[\"SMILES\"],\n",
    "                        \"drug1_desc\": drug_desc_dict[\"Description\"],\n",
    "                        \"protein_desc\": protein_text,\n",
    "                        \"pos\": pos,\n",
    "                        \"neg\": neg\n",
    "                    }\n",
    "                # print(dti_item)\n",
    "                test_results.append(dti_item)\n",
    "    random.shuffle(test_results)\n",
    "    print(\"Original train_length\", len(test_json))\n",
    "    print(\"Filtered train_length\", len(test_results))\n",
    "    writeJSONL(test_results, ou_test_path)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    desc_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp/Drug_description_expand_upload.csv\"\n",
    "    smile_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp/df_drugbank_smiles_filtered.csv\"\n",
    "    protein_desc_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/protein_desc/protein_desc.jsonl\"\n",
    "    \n",
    "    input_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_ori/biosnap/cluster/\"\n",
    "    output_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/cluster_only_correct_0107/fold2/\"\n",
    "    \n",
    "    human_transform(input_dir, output_dir, desc_path, smile_path, protein_desc_path)\n",
    "\n",
    "    print(\"All OK!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a276e5d-c5fe-42aa-beac-06ac9746967e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new length 2198\n",
      "新测试集已创建，包含 2198 条记录.\n"
     ]
    }
   ],
   "source": [
    "#### 数据集筛选\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 train.jsonl 和 test.jsonl 文件路径\n",
    "train_file_path = '/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/random_only/fold2/train.jsonl'\n",
    "test_file_path = '/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/random_only/fold2/test.jsonl'\n",
    "new_test_file_dir = '/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/random_only_exist/fold2/'\n",
    "new_test_data_path = os.path.join(new_test_file_dir, 'test.jsonl')\n",
    "os.makedirs(new_test_file_dir, exist_ok=True)\n",
    "\n",
    "# 从 train.jsonl 读取数据\n",
    "train_ids = set()\n",
    "train_sequences = set()\n",
    "def readJSONL(fp):\n",
    "    res = []\n",
    "    with open(fp,\"r\",encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res  \n",
    "\n",
    "train_json = readJSONL(train_file_path)\n",
    "test_json = readJSONL(test_file_path)\n",
    "for data in train_json:\n",
    "    train_ids.add(data['drug1_id'])\n",
    "    train_sequences.add(data['target_sequence'])\n",
    "\n",
    "# 从 test.jsonl 读取数据并筛选\n",
    "new_test_data = []\n",
    "for data in test_json:\n",
    "    if data['drug1_id'] in train_ids and data['target_sequence'] in train_sequences:\n",
    "        new_test_data.append(data)\n",
    "print(\"new length\", len(new_test_data))\n",
    "# 将筛选后的数据写入新的 test.jsonl 文件\n",
    "def writeJSONL(instance,fp):\n",
    "    with jsonlines.open(fp,'w') as f:\n",
    "        for sample in instance:\n",
    "            f.write(sample)\n",
    "writeJSONL(new_test_data, new_test_data_path)\n",
    "\n",
    "print(f\"新测试集已创建，包含 {len(new_test_data)} 条记录.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69d537fb-f2c7-429c-8966-9bf42dc22e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 8222/8222 [00:00<00:00, 446964.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 8222\n",
      "----------------------------- 开始写arrow文件 ---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4fdf9207b24a0b82f5a13c97dc7f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!!!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from tqdm import tqdm\n",
    "from data_utils import *\n",
    "\n",
    "\n",
    "def make_arrow(data_list, output_dir):\n",
    "    features = Features({\n",
    "        'text': Value('string'),\n",
    "        'text_pos': Value('string'),\n",
    "        'text_neg': Sequence(Value('string')),\n",
    "        'type': Value('string')\n",
    "    })\n",
    "    \n",
    "    # 创建 Arrow 数据集\n",
    "    arrow_dataset = Dataset.from_dict({key: [d[key] for d in data_list] for key in data_list[0]})\n",
    "    \n",
    "    # 保存到磁盘\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    arrow_dataset.save_to_disk(str(output_dir))\n",
    "\n",
    "def tmp2piccolo(root_path, output_dir):\n",
    "    train_json = readJSONL(root_path)\n",
    "\n",
    "    data_list = []\n",
    "    #  {example[\"top1_contents\"]}\n",
    "    for example in tqdm(train_json, desc=\"Processing\"):\n",
    "        if example[\"drug1_id\"]:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug name is {example[\"drug1_name\"]}, the drug smiles is {example[\"drug1_smile\"]} and the drug description is {example[\"drug1_desc\"]}. \n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            \"\"\"\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            text_pos = example[\"pos\"]\n",
    "            text_neg = example[\"neg\"]\n",
    "            type_ = \"cls_contrast\"\n",
    "            \n",
    "            # Append the data as a dictionary\n",
    "            data_list.append({\"text\": text, \"text_pos\": text_pos, \"text_neg\": text_neg, \"type\": type_})\n",
    "    print(\"length\", len(data_list))\n",
    "        # else:\n",
    "        #     text = f\"\"\"\n",
    "        #     Try to figure out drug-target interaction between the drug and the target. \n",
    "        #     The drug smiles is {example[\"drug1_smile\"]}. \n",
    "        #     The target protein sequence is {example[\"target_sequence\"]}.\n",
    "        #     Please think step by step!\n",
    "        #     \"\"\"\n",
    "        #     text = text.replace(\"\\n\", \" \")\n",
    "        #     text_pos = example[\"pos\"]\n",
    "        #     text_neg = example[\"neg\"]\n",
    "        #     type_ = \"cls_contrast\"\n",
    "            \n",
    "        #     # Append the data as a dictionary\n",
    "        #     data_list.append({\"text\": text, \"text_pos\": text_pos, \"text_neg\": text_neg, \"type\": type_})\n",
    "    \n",
    "\n",
    "    print(f\"----------------------------- 开始写arrow文件 ---------------------------------\")\n",
    "    make_arrow(data_list, output_dir)\n",
    "    print(\"OK!!!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_path = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/human_tmp_only_protein/cold_only_retrieval_balanced/fold0/train_add.jsonl\"\n",
    "    output_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_piccolo_all/human_balanced_cold_only_protein_p3/fold0/\"\n",
    "    tmp2piccolo(root_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78ec36-e612-496c-a324-259b04714e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
