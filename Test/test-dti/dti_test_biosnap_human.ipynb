{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5870b4d-dda8-47b0-9c2f-82eeb712ff61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- checkpoint-1280 --------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2198, 1792)\n",
      "embedding完成\n",
      "cm [[ 765  104]\n",
      " [  89 1240]]\n",
      "CSV file /root/autodl-tmp/piccolo-embedding/test-dti/test_results/dti_retri_protein_biosnap_cls2_ml8192_1202.csv has been append.\n",
      "----------------------------- All models OK!!!!! ----------------------\n"
     ]
    }
   ],
   "source": [
    "### biosnap -- random-only-exist\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import util,SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "def cls_test(root_dir, output_dir, model_dir, model_name):\n",
    "    drugbank_test_path = os.path.join(root_dir, \"test_add.jsonl\")\n",
    "    drugbank_test = readJSONL(drugbank_test_path)\n",
    "    ## 改1\n",
    "    ou_drugbank_path = os.path.join(output_dir, \"dti_retri_protein_biosnap_cls2_ml8192_1202.csv\")\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "        \n",
    "    drugbank = []\n",
    "    # The reference chunk is {example[\"top1_contents\"]}, {example[\"top2_contents\"]}, {example[\"top3_contents\"]}.\n",
    "    ## 改2\n",
    "    for example in drugbank_test:\n",
    "        if example[\"drug1_id\"]:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug name is {example[\"drug1_name\"]}, the drug smiles is {example[\"drug1_smile\"]} and the drug description is {example[\"drug1_desc\"]}. \n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            {example[\"top1_contents\"]}\n",
    "            \"\"\"\n",
    "            drugbank.append(text)\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "    pool = model.start_multi_process_pool()\n",
    "    drugbank_embeddings = model.encode_multi_process(drugbank, pool, normalize_embeddings=True, batch_size=100)\n",
    "    print(drugbank_embeddings.shape)\n",
    "    print(\"embedding完成\")\n",
    "    \n",
    "    corpus = ['negative', 'advise']\n",
    "    corpus_embeddings = model.encode_multi_process(corpus, pool, normalize_embeddings=True, batch_size=50)\n",
    "    top_selection = 2\n",
    "    res_drugbank = semantic_search(drugbank_embeddings, corpus_embeddings, query_chunk_size=100, top_k=top_selection, score_function=util.dot_score)\n",
    "\n",
    "    y_pred = np.array([int(res_drugbank[idx][0][\"corpus_id\"]) for idx in range(len(drugbank_test))])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    # Function to compute softmax\n",
    "    def softmax(scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        return exp_scores / exp_scores.sum(axis=0)\n",
    "    \n",
    "    # Iterate through the data and compute softmax, replacing scores\n",
    "    softmax_data = []\n",
    "    for sublist in res_drugbank:\n",
    "        scores = [item['score'] for item in sublist]\n",
    "        softmax_scores = softmax(np.array(scores))\n",
    "        \n",
    "        # Replace scores in the original structure\n",
    "        for i, item in enumerate(sublist):\n",
    "            item['score'] = softmax_scores[i]\n",
    "        softmax_data.append(sublist)\n",
    "    # print(\"softmax_data\", softmax_data)\n",
    "    res_drugbank2 = softmax_data\n",
    "\n",
    "    \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if int(res_drugbank2[idx][0][\"corpus_id\"]) == 0:\n",
    "               y_pred_proba.append(res_drugbank2[idx][1][\"score\"])\n",
    "        else:\n",
    "               y_pred_proba.append(res_drugbank2[idx][0][\"score\"])\n",
    "\n",
    "            \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if example[\"pos\"] == 'negative':\n",
    "            y_true.append(0)\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "           \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    ## drugbank\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('cm', cm)\n",
    "    # # 保存为文本文件\n",
    "    # with open('/root/autodl-tmp/piccolo-embedding/test/test_results/cm.txt', 'w') as f:\n",
    "    #     for row in cm:\n",
    "    #         f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    \n",
    "    drugbank_acc = accuracy_score(y_true, y_pred)\n",
    "    drugbank_pre = precision_score(y_true, y_pred, average='macro')\n",
    "    drugbank_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    drugbank_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    drugbank_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    drugbank_aupr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    ### 改3\n",
    "    new_item_drugbank = {\n",
    "        \"model\": 'dti_ml8192_retri_protein_biosnap_random_only_exist_prompt3_n0_fold2_bs64_add_' + 'stella_' + 'ep10',\n",
    "        \"chem_acc\": drugbank_acc,\n",
    "        \"chem_pre\": drugbank_pre,\n",
    "        \"chem_recall\": drugbank_recall,\n",
    "        \"chem_f1\": drugbank_f1,\n",
    "        \"chem_auc\": drugbank_auc,\n",
    "        \"chem_aupr\": drugbank_aupr\n",
    "    }\n",
    "    if os.path.exists(ou_drugbank_path):\n",
    "        writeCSV_xu([new_item_drugbank], ou_drugbank_path)\n",
    "    else:\n",
    "        writeCSV([new_item_drugbank], ou_drugbank_path)\n",
    "            \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### 改4\n",
    "    root_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/random_only_exist_retrieval/fold2/\"\n",
    "    output_dir = \"/root/autodl-tmp/piccolo-embedding/test-dti/test_results/\"\n",
    "    ## 改5\n",
    "    model_dir = \"/root/autodl-tmp/piccolo-embedding/scripts-dti/formal_biosnap_retri_8192/dti_8192_biosnap_random_only_exist_prompt3_fold2_cls2_add_stella_n0_epoch10_bs64_lr1e5_ml8192_1202/\"\n",
    "    model_names = [\"checkpoint-1280\"]\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"-------------------------------- {model} --------------------------------------\")\n",
    "        cls_test(root_dir, output_dir,model_dir, model)\n",
    "        \n",
    "    print(f\"----------------------------- All models OK!!!!! ----------------------\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ad1710d-5560-4eb2-a371-1d1dbf5f69d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- checkpoint-445 --------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(419, 1792)\n",
      "embedding完成\n",
      "cm [[153  30]\n",
      " [  9 227]]\n",
      "CSV file /root/autodl-tmp/piccolo-embedding/test-dti/test_results/dti_biosnap_ml8192_0324.csv has been created.\n",
      "----------------------------- All models OK!!!!! ----------------------\n"
     ]
    }
   ],
   "source": [
    "##### biosnap -- cluster-only\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import util,SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "def cls_test(root_dir, output_dir, model_dir, model_name):\n",
    "    drugbank_test_path = os.path.join(root_dir, \"test_add.jsonl\")\n",
    "    drugbank_test = readJSONL(drugbank_test_path)\n",
    "    ## 改1\n",
    "    ou_drugbank_path = os.path.join(output_dir, \"dti_biosnap_ml8192_0324.csv\")\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "        \n",
    "    drugbank = []\n",
    "    # The reference chunk is {example[\"top1_contents\"]}, {example[\"top2_contents\"]}, {example[\"top3_contents\"]}.\n",
    "    ## 改2\n",
    "    for example in drugbank_test:\n",
    "        if example[\"drug1_id\"]:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug name is {example[\"drug1_name\"]}, the drug smiles is {example[\"drug1_smile\"]} and the drug description is {example[\"drug1_desc\"]}. \n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            {example[\"top1_contents\"]}\n",
    "            \"\"\"\n",
    "            drugbank.append(text)\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "    pool = model.start_multi_process_pool()\n",
    "    drugbank_embeddings = model.encode_multi_process(drugbank, pool, normalize_embeddings=True, batch_size=100)\n",
    "    print(drugbank_embeddings.shape)\n",
    "    print(\"embedding完成\")\n",
    "    \n",
    "    corpus = ['negative', 'advise']\n",
    "    corpus_embeddings = model.encode_multi_process(corpus, pool, normalize_embeddings=True, batch_size=50)\n",
    "    top_selection = 2\n",
    "    res_drugbank = semantic_search(drugbank_embeddings, corpus_embeddings, query_chunk_size=100, top_k=top_selection, score_function=util.dot_score)\n",
    "\n",
    "    y_pred = np.array([int(res_drugbank[idx][0][\"corpus_id\"]) for idx in range(len(drugbank_test))])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    # Function to compute softmax\n",
    "    def softmax(scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        return exp_scores / exp_scores.sum(axis=0)\n",
    "    \n",
    "    # Iterate through the data and compute softmax, replacing scores\n",
    "    softmax_data = []\n",
    "    for sublist in res_drugbank:\n",
    "        scores = [item['score'] for item in sublist]\n",
    "        softmax_scores = softmax(np.array(scores))\n",
    "        \n",
    "        # Replace scores in the original structure\n",
    "        for i, item in enumerate(sublist):\n",
    "            item['score'] = softmax_scores[i]\n",
    "        softmax_data.append(sublist)\n",
    "    # print(\"softmax_data\", softmax_data)\n",
    "    res_drugbank2 = softmax_data\n",
    "\n",
    "    \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if int(res_drugbank2[idx][0][\"corpus_id\"]) == 0:\n",
    "               y_pred_proba.append(res_drugbank2[idx][1][\"score\"])\n",
    "        else:\n",
    "               y_pred_proba.append(res_drugbank2[idx][0][\"score\"])\n",
    "\n",
    "            \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if example[\"pos\"] == 'negative':\n",
    "            y_true.append(0)\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "           \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    ## drugbank\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('cm', cm)\n",
    "    # # 保存为文本文件\n",
    "    # with open('/root/autodl-tmp/piccolo-embedding/test/test_results/cm.txt', 'w') as f:\n",
    "    #     for row in cm:\n",
    "    #         f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    \n",
    "    drugbank_acc = accuracy_score(y_true, y_pred)\n",
    "    drugbank_pre = precision_score(y_true, y_pred, average='macro')\n",
    "    drugbank_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    drugbank_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    drugbank_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    drugbank_aupr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    ### 改3\n",
    "    new_item_drugbank = {\n",
    "        \"model\": 'dti_ml8192_biosnap_cluster_only_n0_fold0_add_' + 'stella_' + 'ep?',\n",
    "        \"chem_acc\": drugbank_acc,\n",
    "        \"chem_pre\": drugbank_pre,\n",
    "        \"chem_recall\": drugbank_recall,\n",
    "        \"chem_f1\": drugbank_f1,\n",
    "        \"chem_auc\": drugbank_auc,\n",
    "        \"chem_aupr\": drugbank_aupr\n",
    "    }\n",
    "    if os.path.exists(ou_drugbank_path):\n",
    "        writeCSV_xu([new_item_drugbank], ou_drugbank_path)\n",
    "    else:\n",
    "        writeCSV([new_item_drugbank], ou_drugbank_path)\n",
    "            \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### 改4\n",
    "    root_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/cluster_only_retrieval/fold0/\"\n",
    "    output_dir = \"/root/autodl-tmp/piccolo-embedding/test-dti/test_results/\"\n",
    "    ## 改5\n",
    "    model_dir = \"/root/autodl-tmp/piccolo-embedding/scripts-dti/formal_biosnap_retri_8192/dti_8192_biosnap_cluster_only_prompt3_fold0_cls2_add_stella_n0_epoch15_bs64_lr1e5_ml8192_1202/\"\n",
    "    model_names = [\"checkpoint-445\"]\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"-------------------------------- {model} --------------------------------------\")\n",
    "        cls_test(root_dir, output_dir,model_dir, model)\n",
    "        \n",
    "    print(f\"----------------------------- All models OK!!!!! ----------------------\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acdc2985-0516-4ec1-90c9-5c6592e444cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- checkpoint-240 --------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9899, 1792)\n",
      "embedding完成\n",
      "cm [[1814 2208]\n",
      " [ 364 5513]]\n",
      "CSV file /root/autodl-tmp/piccolo-embedding/test-dti/test_results/dti_biosnap_ml8192_0324-2.csv has been append.\n",
      "----------------------------- All models OK!!!!! ----------------------\n"
     ]
    }
   ],
   "source": [
    "### biosnap -- scarce\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import util,SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "def cls_test(root_dir, output_dir, model_dir, model_name):\n",
    "    drugbank_test_path = os.path.join(root_dir, \"test_add.jsonl\")\n",
    "    drugbank_test = readJSONL(drugbank_test_path)\n",
    "    ## 改1\n",
    "    ou_drugbank_path = os.path.join(output_dir, \"dti_biosnap_ml8192_0324-2.csv\")\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "        \n",
    "    drugbank = []\n",
    "    # The reference chunk is {example[\"top1_contents\"]}, {example[\"top2_contents\"]}, {example[\"top3_contents\"]}.\n",
    "    ## 改2\n",
    "    for example in drugbank_test:\n",
    "        if example[\"drug1_id\"]:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug name is {example[\"drug1_name\"]}, the drug smiles is {example[\"drug1_smile\"]} and the drug description is {example[\"drug1_desc\"]}. \n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            {example[\"top1_contents\"]}\n",
    "            \"\"\"\n",
    "            drugbank.append(text)\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "    pool = model.start_multi_process_pool()\n",
    "    drugbank_embeddings = model.encode_multi_process(drugbank, pool, normalize_embeddings=True, batch_size=50)\n",
    "    print(drugbank_embeddings.shape)\n",
    "    print(\"embedding完成\")\n",
    "    \n",
    "    corpus = ['negative', 'advise']\n",
    "    corpus_embeddings = model.encode_multi_process(corpus, pool, normalize_embeddings=True, batch_size=50)\n",
    "    top_selection = 2\n",
    "    res_drugbank = semantic_search(drugbank_embeddings, corpus_embeddings, query_chunk_size=100, top_k=top_selection, score_function=util.dot_score)\n",
    "\n",
    "    y_pred = np.array([int(res_drugbank[idx][0][\"corpus_id\"]) for idx in range(len(drugbank_test))])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    # Function to compute softmax\n",
    "    def softmax(scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        return exp_scores / exp_scores.sum(axis=0)\n",
    "    \n",
    "    # Iterate through the data and compute softmax, replacing scores\n",
    "    softmax_data = []\n",
    "    for sublist in res_drugbank:\n",
    "        scores = [item['score'] for item in sublist]\n",
    "        softmax_scores = softmax(np.array(scores))\n",
    "        \n",
    "        # Replace scores in the original structure\n",
    "        for i, item in enumerate(sublist):\n",
    "            item['score'] = softmax_scores[i]\n",
    "        softmax_data.append(sublist)\n",
    "    # print(\"softmax_data\", softmax_data)\n",
    "    res_drugbank2 = softmax_data\n",
    "\n",
    "    \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if int(res_drugbank2[idx][0][\"corpus_id\"]) == 0:\n",
    "               y_pred_proba.append(res_drugbank2[idx][1][\"score\"])\n",
    "        else:\n",
    "               y_pred_proba.append(res_drugbank2[idx][0][\"score\"])\n",
    "\n",
    "            \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if example[\"pos\"] == 'negative':\n",
    "            y_true.append(0)\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "           \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    ## drugbank\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('cm', cm)\n",
    "    # # 保存为文本文件\n",
    "    # with open('/root/autodl-tmp/piccolo-embedding/test/test_results/cm.txt', 'w') as f:\n",
    "    #     for row in cm:\n",
    "    #         f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    \n",
    "    drugbank_acc = accuracy_score(y_true, y_pred)\n",
    "    drugbank_pre = precision_score(y_true, y_pred, average='macro')\n",
    "    drugbank_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    drugbank_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    drugbank_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    drugbank_aupr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    ### 改3\n",
    "    new_item_drugbank = {\n",
    "        \"model\": 'dti_ml8192_biosnap_scarce_5_n0_fold2_add_' + 'stella_' + 'ep30',\n",
    "        \"chem_acc\": drugbank_acc,\n",
    "        \"chem_pre\": drugbank_pre,\n",
    "        \"chem_recall\": drugbank_recall,\n",
    "        \"chem_f1\": drugbank_f1,\n",
    "        \"chem_auc\": drugbank_auc,\n",
    "        \"chem_aupr\": drugbank_aupr\n",
    "    }\n",
    "    if os.path.exists(ou_drugbank_path):\n",
    "        writeCSV_xu([new_item_drugbank], ou_drugbank_path)\n",
    "    else:\n",
    "        writeCSV([new_item_drugbank], ou_drugbank_path)\n",
    "            \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### 改4\n",
    "    root_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/biosnap_tmp/scarce_exist_retrieval/5/fold2/\"\n",
    "    output_dir = \"/root/autodl-tmp/piccolo-embedding/test-dti/test_results/\"\n",
    "    ## 改5\n",
    "    model_dir = \"/root/autodl-tmp/piccolo-embedding/scripts-dti/formal_biosnap_retri_8192/dti_8192_biosnap_scarce_5_fold2_n0_epoch40_bs64_lr1e5_0324/\"\n",
    "    model_names = [\"checkpoint-240\"]\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"-------------------------------- {model} --------------------------------------\")\n",
    "        cls_test(root_dir, output_dir,model_dir, model)\n",
    "        \n",
    "    print(f\"----------------------------- All models OK!!!!! ----------------------\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5223440-781a-497a-8f3c-8f72def6a83b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- checkpoint-630 --------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949, 1792)\n",
      "embedding完成\n",
      "cm [[481  25]\n",
      " [ 25 418]]\n",
      "CSV file /root/autodl-tmp/piccolo-embedding/test-dti/test_results/dti_retri_protein_human_cls2_ml8192_1127.csv has been append.\n",
      "----------------------------- All models OK!!!!! ----------------------\n"
     ]
    }
   ],
   "source": [
    "### human -- random-only-exist\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import util,SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "def cls_test(root_dir, output_dir, model_dir, model_name):\n",
    "    drugbank_test_path = os.path.join(root_dir, \"test_add.jsonl\")\n",
    "    drugbank_test = readJSONL(drugbank_test_path)\n",
    "    ## 改1\n",
    "    ou_drugbank_path = os.path.join(output_dir, \"dti_retri_protein_human_cls2_ml8192_1127.csv\")\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "        \n",
    "    drugbank = []\n",
    "    # The reference chunk is {example[\"top1_contents\"]}, {example[\"top2_contents\"]}, {example[\"top3_contents\"]}.\n",
    "    ## 改2\n",
    "    for example in drugbank_test:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug smiles is {example[\"drug1_smile\"]}\n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            {example[\"top1_contents\"]}\n",
    "            \"\"\"\n",
    "            drugbank.append(text)\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "    pool = model.start_multi_process_pool()\n",
    "    drugbank_embeddings = model.encode_multi_process(drugbank, pool, normalize_embeddings=True, batch_size=100)\n",
    "    print(drugbank_embeddings.shape)\n",
    "    print(\"embedding完成\")\n",
    "    \n",
    "    corpus = ['negative', 'advise']\n",
    "    corpus_embeddings = model.encode_multi_process(corpus, pool, normalize_embeddings=True, batch_size=50)\n",
    "    top_selection = 2\n",
    "    res_drugbank = semantic_search(drugbank_embeddings, corpus_embeddings, query_chunk_size=100, top_k=top_selection, score_function=util.dot_score)\n",
    "\n",
    "    y_pred = np.array([int(res_drugbank[idx][0][\"corpus_id\"]) for idx in range(len(drugbank_test))])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    # Function to compute softmax\n",
    "    def softmax(scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        return exp_scores / exp_scores.sum(axis=0)\n",
    "    \n",
    "    # Iterate through the data and compute softmax, replacing scores\n",
    "    softmax_data = []\n",
    "    for sublist in res_drugbank:\n",
    "        scores = [item['score'] for item in sublist]\n",
    "        softmax_scores = softmax(np.array(scores))\n",
    "        \n",
    "        # Replace scores in the original structure\n",
    "        for i, item in enumerate(sublist):\n",
    "            item['score'] = softmax_scores[i]\n",
    "        softmax_data.append(sublist)\n",
    "    # print(\"softmax_data\", softmax_data)\n",
    "    res_drugbank2 = softmax_data\n",
    "\n",
    "    \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if int(res_drugbank2[idx][0][\"corpus_id\"]) == 0:\n",
    "               y_pred_proba.append(res_drugbank2[idx][1][\"score\"])\n",
    "        else:\n",
    "               y_pred_proba.append(res_drugbank2[idx][0][\"score\"])\n",
    "\n",
    "            \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if example[\"pos\"] == 'negative':\n",
    "            y_true.append(0)\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "           \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    ## drugbank\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('cm', cm)\n",
    "    # # 保存为文本文件\n",
    "    # with open('/root/autodl-tmp/piccolo-embedding/test/test_results/cm.txt', 'w') as f:\n",
    "    #     for row in cm:\n",
    "    #         f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    \n",
    "    drugbank_acc = accuracy_score(y_true, y_pred)\n",
    "    drugbank_pre = precision_score(y_true, y_pred, average='macro')\n",
    "    drugbank_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    drugbank_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    drugbank_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    drugbank_aupr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    ### 改3\n",
    "    new_item_drugbank = {\n",
    "        \"model\": 'dti_human_random_only_exist_prompt3_n0_fold2_bs64_retrain_add_' + 'stella_' + 'ep10',\n",
    "        \"chem_acc\": drugbank_acc,\n",
    "        \"chem_pre\": drugbank_pre,\n",
    "        \"chem_recall\": drugbank_recall,\n",
    "        \"chem_f1\": drugbank_f1,\n",
    "        \"chem_auc\": drugbank_auc,\n",
    "        \"chem_aupr\": drugbank_aupr\n",
    "    }\n",
    "    if os.path.exists(ou_drugbank_path):\n",
    "        writeCSV_xu([new_item_drugbank], ou_drugbank_path)\n",
    "    else:\n",
    "        writeCSV([new_item_drugbank], ou_drugbank_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### 改4\n",
    "    root_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/human_tmp_only_protein/random_only_exist_retrieval/fold2/\"\n",
    "    output_dir = \"/root/autodl-tmp/piccolo-embedding/test-dti/test_results/\"\n",
    "    ## 改5\n",
    "    model_dir = \"/root/autodl-tmp/piccolo-embedding/scripts-dti/formal_human_retri_8192/dti_8192_human_random_only_exist_prompt3_fold2_cls2_add_stella_n0_epoch10_bs64_lr1e5_ml8192_1202/\"\n",
    "    model_names = [\"checkpoint-630\"]\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"-------------------------------- {model} --------------------------------------\")\n",
    "        cls_test(root_dir, output_dir,model_dir, model)\n",
    "        \n",
    "    print(f\"----------------------------- All models OK!!!!! ----------------------\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb74b23-628b-43f5-984c-db673b407fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------- checkpoint-245 --------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 1792)\n",
      "embedding完成\n",
      "cm [[108  12]\n",
      " [ 60  60]]\n",
      "csv输出完成！！\n",
      "CSV file /root/autodl-tmp/piccolo-embedding/test-dti/test_results/dti_retri_protein_human_cls2_ml8192_1127.csv has been append.\n",
      "----------------------------- All models OK!!!!! ----------------------\n"
     ]
    }
   ],
   "source": [
    "### human cold-balanced\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from data_utils import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sentence_transformers import util,SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "def cls_test(root_dir, output_dir, model_dir, model_name):\n",
    "    drugbank_test_path = os.path.join(root_dir, \"test_add.jsonl\")\n",
    "    drugbank_test = readJSONL(drugbank_test_path)\n",
    "    ## 改1\n",
    "    ou_drugbank_path = os.path.join(output_dir, \"dti_retri_protein_human_cls2_ml8192_1127.csv\")\n",
    "    model_path = os.path.join(model_dir, model_name)\n",
    "    \n",
    "        \n",
    "    drugbank = []\n",
    "    # The reference chunk is {example[\"top1_contents\"]}, {example[\"top2_contents\"]}, {example[\"top3_contents\"]}.\n",
    "    ## 改2\n",
    "    for example in drugbank_test:\n",
    "            text = f\"\"\"\n",
    "            Try to figure out drug-target interaction between the drug and the target. \n",
    "            The drug smiles is {example[\"drug1_smile\"]}\n",
    "            The target protein description is {example[\"protein_desc\"]}\n",
    "            Please think step by step!\n",
    "            {example[\"top1_contents\"]}\n",
    "            \"\"\"\n",
    "            drugbank.append(text)\n",
    "\n",
    "\n",
    "    model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "    pool = model.start_multi_process_pool()\n",
    "    drugbank_embeddings = model.encode_multi_process(drugbank, pool, normalize_embeddings=True, batch_size=100)\n",
    "    print(drugbank_embeddings.shape)\n",
    "    print(\"embedding完成\")\n",
    "    \n",
    "    corpus = ['negative', 'advise']\n",
    "    corpus_embeddings = model.encode_multi_process(corpus, pool, normalize_embeddings=True, batch_size=50)\n",
    "    top_selection = 2\n",
    "    res_drugbank = semantic_search(drugbank_embeddings, corpus_embeddings, query_chunk_size=100, top_k=top_selection, score_function=util.dot_score)\n",
    "\n",
    "    y_pred = np.array([int(res_drugbank[idx][0][\"corpus_id\"]) for idx in range(len(drugbank_test))])\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    # Function to compute softmax\n",
    "    def softmax(scores):\n",
    "        exp_scores = np.exp(scores - np.max(scores))  # Subtract max for numerical stability\n",
    "        return exp_scores / exp_scores.sum(axis=0)\n",
    "    \n",
    "    # Iterate through the data and compute softmax, replacing scores\n",
    "    softmax_data = []\n",
    "    for sublist in res_drugbank:\n",
    "        scores = [item['score'] for item in sublist]\n",
    "        softmax_scores = softmax(np.array(scores))\n",
    "        \n",
    "        # Replace scores in the original structure\n",
    "        for i, item in enumerate(sublist):\n",
    "            item['score'] = softmax_scores[i]\n",
    "        softmax_data.append(sublist)\n",
    "    # print(\"softmax_data\", softmax_data)\n",
    "    res_drugbank2 = softmax_data\n",
    "\n",
    "    \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if int(res_drugbank2[idx][0][\"corpus_id\"]) == 0:\n",
    "               y_pred_proba.append(res_drugbank2[idx][1][\"score\"])\n",
    "        else:\n",
    "               y_pred_proba.append(res_drugbank2[idx][0][\"score\"])\n",
    "\n",
    "            \n",
    "    for idx, example in enumerate(drugbank_test):\n",
    "        if example[\"pos\"] == 'negative':\n",
    "            y_true.append(0)\n",
    "        else:\n",
    "            y_true.append(1)\n",
    "           \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    ## drugbank\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print('cm', cm)\n",
    "    # # 保存为文本文件\n",
    "    # with open('/root/autodl-tmp/piccolo-embedding/test/test_results/cm.txt', 'w') as f:\n",
    "    #     for row in cm:\n",
    "    #         f.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "    \n",
    "    drugbank_acc = accuracy_score(y_true, y_pred)\n",
    "    drugbank_pre = precision_score(y_true, y_pred, average='macro')\n",
    "    drugbank_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    drugbank_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    drugbank_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    drugbank_aupr = average_precision_score(y_true, y_pred_proba)\n",
    "\n",
    "    # 保存y_label和y_pred到CSV文件\n",
    "    ## 自添加\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_labels': y_true,\n",
    "        'predicted_prob': y_pred_proba\n",
    "    })\n",
    "    results_df.to_csv('/root/autodl-tmp/piccolo-embedding/test-dti/test_results/cold-csv/cold-predictions.csv', index=False)\n",
    "    print('csv输出完成！！')\n",
    "\n",
    "    \n",
    "\n",
    "    ### 改3\n",
    "    new_item_drugbank = {\n",
    "        \"model\": 'dti_human_balanced_cold_only_prompt3_n0_fold2_bs64_retrain_add_' + 'stella_' + 'ep5',\n",
    "        \"chem_acc\": drugbank_acc,\n",
    "        \"chem_pre\": drugbank_pre,\n",
    "        \"chem_recall\": drugbank_recall,\n",
    "        \"chem_f1\": drugbank_f1,\n",
    "        \"chem_auc\": drugbank_auc,\n",
    "        \"chem_aupr\": drugbank_aupr\n",
    "    }\n",
    "    if os.path.exists(ou_drugbank_path):\n",
    "        writeCSV_xu([new_item_drugbank], ou_drugbank_path)\n",
    "    else:\n",
    "        writeCSV([new_item_drugbank], ou_drugbank_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### 改4\n",
    "    root_dir = \"/root/autodl-tmp/dataset_dti/datasets/datasets_tmp_all/human_tmp_only_protein/cold_only_retrieval_balanced/fold2/\"\n",
    "    output_dir = \"/root/autodl-tmp/piccolo-embedding/test-dti/test_results/\"\n",
    "    ## 改5\n",
    "    model_dir = \"/root/autodl-tmp/piccolo-embedding/scripts-dti/formal_human_retri_8192/dti_8192_human_balanced_cold_only_prompt3_fold2_cls2_add_stella_n0_epoch5_bs64_lr1e5_ml8192_1202/\"\n",
    "    model_names = [\"checkpoint-245\"]\n",
    "    \n",
    "    for model in model_names:\n",
    "        print(f\"-------------------------------- {model} --------------------------------------\")\n",
    "        cls_test(root_dir, output_dir,model_dir, model)\n",
    "        \n",
    "    print(f\"----------------------------- All models OK!!!!! ----------------------\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67304151-2114-4265-8e89-748ba6970c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
